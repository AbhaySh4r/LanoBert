{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The sky turned a fiery orange as the sun dipped below the horizon, casting long shadows across the tranquil meadow.\",\n",
    "    \"Lost in thought, she wandered through the labyrinth of bustling streets, each alleyway revealing a new adventure waiting to unfold.\",\n",
    "    \"With a flicker of hesitation, he reached out and grasped the ancient artifact, feeling its power surge through his veins.\",\n",
    "    \"Laughter echoed through the halls as friends gathered around the crackling fireplace, swapping stories late into the night.\",\n",
    "    \"Time seemed to stand still as they danced beneath the twinkling stars, their hearts beating in perfect harmony.\",\n",
    "    \"The aroma of freshly brewed coffee filled the air, awakening her senses with each comforting sip.\",\n",
    "    \"Thunder rumbled ominously in the distance, signaling an approaching storm that would soon unleash its fury upon the unsuspecting town.\",\n",
    "    \"In the quiet solitude of the forest, she found solace among the towering trees, their gentle whispers soothing her troubled mind.\",\n",
    "    \"With a flourish of his pen, he signed his name at the bottom of the contract, sealing the deal with a sense of satisfaction.\",\n",
    "    \"As dawn broke over the horizon, painting the sky in hues of pink and gold, she knew that today held endless possibilities waiting to be discovered.\",\n",
    "    \"This is an anomaly, This is an anomaly. This. Is definitely an Anomaly. Anomaly is what this is\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No FineTune, Stock BERT MLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [tokenizer(sentence, add_special_tokens=True, padding='max_length', truncation=True, max_length=32, return_tensors='pt') for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(tokenized_sentences, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "KEY = []\n",
    "DICT = {}\n",
    "\n",
    "def abn_loss(loss : list, k):\n",
    "    scores = torch.topk(torch.Tensor(loss), k)\n",
    "    return (sum(scores.values.tolist()))/k\n",
    "\n",
    "def abn_prob(probs, k):\n",
    "    scores = torch.topk(torch.Tensor(probs), k, largest=False)\n",
    "    return (sum(scores.values.tolist()))/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.812236944834392 0.13666570434967676\n",
      "3.6685471534729004 0.10489523783326149\n",
      "5.667535463968913 0.10991307844718297\n",
      "6.012920538584392 0.07328140611449878\n",
      "6.540809154510498 0.09861249476671219\n",
      "8.561833381652832 0.13112881034612656\n",
      "2.7490862210591636 0.12969542170564333\n",
      "4.370119730631511 0.07640538861354192\n",
      "2.7414368788401284 0.12090899546941121\n",
      "1.237165093421936 0.167165690412124\n",
      "7.45561949412028 0.3833014965057373\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "for n in np.arange(0, len(tokenized_sentences)):\n",
    "    data = tokenized_sentences[n]\n",
    "    phrase = data.input_ids\n",
    "    data['labels'] = phrase.clone()\n",
    "    if phrase not in DICT:\n",
    "        score_loss, score_prob = [], []\n",
    "        with torch.no_grad():        \n",
    "            for i in np.arange(1, len(phrase[0])):\n",
    "                temp = phrase[0][i].item()\n",
    "                phrase[0][i] = 103\n",
    "\n",
    "                outputs = model(**data)\n",
    "\n",
    "                outputs_prob = max(torch.nn.functional.softmax(outputs.logits[0][i], dim=0))\n",
    "\n",
    "                score_loss.append(outputs.loss)\n",
    "                score_prob.append(outputs_prob)\n",
    "\n",
    "                #print(outputs.loss, outputs_prob)\n",
    "\n",
    "                \n",
    "                phrase[0][i] = temp\n",
    "        \n",
    "        agg_loss = abn_loss(score_loss, 3)\n",
    "        agg_prob = abn_prob(score_prob, 3)\n",
    "        #agg_prob = abn_prob(outputs.loss, 5)\n",
    "        KEY.append(phrase)\n",
    "        DICT.update({phrase : (agg_loss, agg_prob)})\n",
    "\n",
    "        print(agg_loss, agg_prob)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print('From Database', DICT[phrase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = DICT.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score 6.732313281330046\n",
      "Anomaly Score 4.098455631710213\n",
      "Anomaly Score 6.367395505690066\n",
      "Anomaly Score 6.488399583495683\n",
      "Anomaly Score 7.256378767772772\n",
      "Anomaly Score 9.8539731592016\n",
      "Anomaly Score 3.1587633681611638\n",
      "Anomaly Score 4.731642732379401\n",
      "Anomaly Score 3.1184904233026285\n",
      "Anomaly Score 1.4854876644480957\n",
      "Anomaly Score 12.089569622556482\n"
     ]
    }
   ],
   "source": [
    "for i, score in enumerate(scores):\n",
    "    anomaly_score = score[0] * 1/(1-score[1])\n",
    "    print('Anomaly Score', anomaly_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1996,  3712,  2357,  1037, 15443,  4589,  2004,  1996,  3103,\n",
       "           103,  2917,  1996,  9154,  1010,  9179,  2146,  6281,  2408,  1996,\n",
       "         25283, 26147, 13244,  1012,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[  101,  1996,  3712,  2357,  1037, 15443,  4589,  2004,  1996,  3103,\n",
       "         13537,  2917,  1996,  9154,  1010,  9179,  2146,  6281,  2408,  1996,\n",
       "         25283, 26147, 13244,  1012,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0]])}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tokenized_sentences[0]\n",
    "test['labels'] = test.input_ids.detach().clone()\n",
    "test.input_ids[0, 10] = 103\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6932, grad_fn=<UnbindBackward0>)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**test)\n",
    "#print(outputs.loss)\n",
    "#print(outputs.logits.shape)\n",
    "#print(outputs.logits[0][10])\n",
    "proba = torch.nn.functional.softmax(outputs.logits[0][10], dim=0)\n",
    "print(max(proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9999, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(torch.nn.functional.softmax(outputs.logits[0][10], dim = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
