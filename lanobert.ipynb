{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The sky turned a fiery orange as the sun dipped below the horizon, casting long shadows across the tranquil meadow.\",\n",
    "    \"Lost in thought, she wandered through the labyrinth of bustling streets, each alleyway revealing a new adventure waiting to unfold.\",\n",
    "    \"With a flicker of hesitation, he reached out and grasped the ancient artifact, feeling its power surge through his veins.\",\n",
    "    \"Laughter echoed through the halls as friends gathered around the crackling fireplace, swapping stories late into the night.\",\n",
    "    \"Time seemed to stand still as they danced beneath the twinkling stars, their hearts beating in perfect harmony.\",\n",
    "    \"The aroma of freshly brewed coffee filled the air, awakening her senses with each comforting sip.\",\n",
    "    \"Thunder rumbled ominously in the distance, signaling an approaching storm that would soon unleash its fury upon the unsuspecting town.\",\n",
    "    \"In the quiet solitude of the forest, she found solace among the towering trees, their gentle whispers soothing her troubled mind.\",\n",
    "    \"With a flourish of his pen, he signed his name at the bottom of the contract, sealing the deal with a sense of satisfaction.\",\n",
    "    \"As dawn broke over the horizon, painting the sky in hues of pink and gold, she knew that today held endless possibilities waiting to be discovered.\",\n",
    "    \"This is anomaly :)\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No FineTune, Stock BERT MLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [tokenizer(sentence, add_special_tokens=True, padding='max_length', truncation=True, max_length=32, return_tensors='pt') for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[  101,  1996,  3712,  2357,  1037, 15443,  4589,  2004,  1996,  3103,\n",
       "          13537,  2917,  1996,  9154,  1010,  9179,  2146,  6281,  2408,  1996,\n",
       "          25283, 26147, 13244,  1012,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2439,  1999,  2245,  1010,  2016, 13289,  2083,  1996, 24239,\n",
       "           1997, 13950,  2989,  4534,  1010,  2169,  8975,  4576,  8669,  1037,\n",
       "           2047,  6172,  3403,  2000,  4895, 10371,  1012,   102,     0,     0,\n",
       "              0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 0, 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2007,  1037, 17909,  1997, 13431,  1010,  2002,  2584,  2041,\n",
       "           1998, 15517,  1996,  3418, 20785,  1010,  3110,  2049,  2373, 12058,\n",
       "           2083,  2010,  9607,  1012,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  7239, 10187,  2083,  1996,  9873,  2004,  2814,  5935,  2105,\n",
       "           1996,  8579,  2989, 13788,  1010, 19948,  4691,  3441,  2397,  2046,\n",
       "           1996,  2305,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2051,  2790,  2000,  3233,  2145,  2004,  2027, 10948,  4218,\n",
       "           1996,  5519, 20260,  3340,  1010,  2037,  8072,  6012,  1999,  3819,\n",
       "           9396,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  1996, 23958,  1997, 20229, 24702,  2098,  4157,  3561,  1996,\n",
       "           2250,  1010, 16936,  2014,  9456,  2007,  2169, 16334, 10668,  1012,\n",
       "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  8505, 22257, 23504,  2135,  1999,  1996,  3292,  1010, 14828,\n",
       "           2019,  8455,  4040,  2008,  2052,  2574,  4895, 19738,  4095,  2049,\n",
       "           8111,  2588,  1996,  4895, 13203,  5051, 11873,  2237,  1012,   102,\n",
       "              0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  1999,  1996,  4251, 22560,  1997,  1996,  3224,  1010,  2016,\n",
       "           2179, 14017, 10732,  2426,  1996, 20314,  3628,  1010,  2037,  7132,\n",
       "          11054, 16684,  2014, 11587,  2568,  1012,   102,     0,     0,     0,\n",
       "              0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 0, 0, 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2007,  1037, 24299,  1997,  2010,  7279,  1010,  2002,  2772,\n",
       "           2010,  2171,  2012,  1996,  3953,  1997,  1996,  3206,  1010, 23038,\n",
       "           1996,  3066,  2007,  1037,  3168,  1997,  9967,  1012,   102,     0,\n",
       "              0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2004,  6440,  3631,  2058,  1996,  9154,  1010,  4169,  1996,\n",
       "           3712,  1999, 20639,  2015,  1997,  5061,  1998,  2751,  1010,  2016,\n",
       "           2354,  2008,  2651,  2218, 10866, 12020,  3403,  2000,  2022,  3603,\n",
       "           1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[  101,  2023,  2003, 28685,  1024,  1007,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]])}]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "KEY = []\n",
    "DICT = {}\n",
    "\n",
    "def abn_loss(loss : list, k):\n",
    "    scores = torch.topk(torch.Tensor(loss), k)\n",
    "    return sum(scores.values.tolist())\n",
    "\n",
    "def abn_prob(probs, k):\n",
    "    scores = torch.topk(torch.Tensor(probs), k, largest=False)\n",
    "    return sum(scores.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.216364860534668 0.35726476460695267\n",
      "11.005641460418701 0.31468571349978447\n",
      "17.00260639190674 0.3297392353415489\n",
      "18.038761615753174 0.21984421834349632\n",
      "19.622427463531494 0.29583748430013657\n",
      "25.685500144958496 0.39338643103837967\n",
      "8.24725866317749 0.38908626511693\n",
      "13.110359191894531 0.22921616584062576\n",
      "8.224310636520386 0.36272698640823364\n",
      "3.711495280265808 0.501497071236372\n",
      "50.586917877197266 0.5157705321907997\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "for n in np.arange(0, len(tokenized_sentences)):\n",
    "    data = tokenized_sentences[n]\n",
    "    phrase = data.input_ids\n",
    "    data['labels'] = phrase.clone()\n",
    "    if phrase not in DICT:\n",
    "        score_loss, score_prob = [], []        \n",
    "        for i in np.arange(1, len(phrase[0])):\n",
    "            temp = phrase[0][i].item()\n",
    "            phrase[0][i] = 103\n",
    "\n",
    "            outputs = model(**data)\n",
    "\n",
    "            outputs_prob = max(torch.nn.functional.softmax(outputs.logits[0][i], dim=0))\n",
    "\n",
    "            score_loss.append(outputs.loss)\n",
    "            score_prob.append(outputs_prob)\n",
    "\n",
    "            #print(outputs.loss, outputs_prob)\n",
    "\n",
    "            \n",
    "            phrase[0][i] = temp\n",
    "        \n",
    "        agg_loss = abn_loss(score_loss, 3)\n",
    "        agg_prob = abn_prob(score_prob, 3)\n",
    "        #agg_prob = abn_prob(outputs.loss, 5)\n",
    "        KEY.append(phrase)\n",
    "        DICT.update({phrase : (agg_loss, agg_prob)})\n",
    "\n",
    "        print(agg_loss, agg_prob)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print('else')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([(22.216364860534668, 0.35726476460695267), (11.005641460418701, 0.31468571349978447), (17.00260639190674, 0.3297392353415489), (18.038761615753174, 0.21984421834349632), (19.622427463531494, 0.29583748430013657), (25.685500144958496, 0.39338643103837967), (8.24725866317749, 0.38908626511693), (13.110359191894531, 0.22921616584062576), (8.224310636520386, 0.36272698640823364), (3.711495280265808, 0.501497071236372), (50.586917877197266, 0.5157705321907997)])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DICT.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1996,  3712,  2357,  1037, 15443,  4589,  2004,  1996,  3103,\n",
       "           103,  2917,  1996,  9154,  1010,  9179,  2146,  6281,  2408,  1996,\n",
       "         25283, 26147, 13244,  1012,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[  101,  1996,  3712,  2357,  1037, 15443,  4589,  2004,  1996,  3103,\n",
       "         13537,  2917,  1996,  9154,  1010,  9179,  2146,  6281,  2408,  1996,\n",
       "         25283, 26147, 13244,  1012,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0]])}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tokenized_sentences[0]\n",
    "test['labels'] = test.input_ids.detach().clone()\n",
    "test.input_ids[0, 10] = 103\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[262], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#print(outputs.loss)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#print(outputs.logits.shape)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#print(outputs.logits[0][10])\u001b[39;00m\n\u001b[0;32m      5\u001b[0m proba \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(outputs\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m10\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mproba\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "outputs = model(**test)\n",
    "#print(outputs.loss)\n",
    "#print(outputs.logits.shape)\n",
    "#print(outputs.logits[0][10])\n",
    "proba = torch.nn.functional.softmax(outputs.logits[0][10], dim=0)\n",
    "print(max(proba,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9999, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(torch.nn.functional.softmax(outputs.logits[0][10], dim = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
