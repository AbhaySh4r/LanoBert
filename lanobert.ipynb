{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The sky turned a fiery orange as the sun dipped below the horizon, casting long shadows across the tranquil meadow.\",\n",
    "    \"Lost in thought, she wandered through the labyrinth of bustling streets, each alleyway revealing a new adventure waiting to unfold.\",\n",
    "    \"With a flicker of hesitation, he reached out and grasped the ancient artifact, feeling its power surge through his veins.\",\n",
    "    \"Laughter echoed through the halls as friends gathered around the crackling fireplace, swapping stories late into the night.\",\n",
    "    \"Time seemed to stand still as they danced beneath the twinkling stars, their hearts beating in perfect harmony.\",\n",
    "    \"The aroma of freshly brewed coffee filled the air, awakening her senses with each comforting sip.\",\n",
    "    \"Thunder rumbled ominously in the distance, signaling an approaching storm that would soon unleash its fury upon the unsuspecting town.\",\n",
    "    \"In the quiet solitude of the forest, she found solace among the towering trees, their gentle whispers soothing her troubled mind.\",\n",
    "    \"With a flourish of his pen, he signed his name at the bottom of the contract, sealing the deal with a sense of satisfaction.\",\n",
    "    \"As dawn broke over the horizon, painting the sky in hues of pink and gold, she knew that today held endless possibilities waiting to be discovered.\",\n",
    "    \"This is anomaly :)\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No FineTune, Stock BERT MLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [tokenizer(sentence, add_special_tokens=True, padding='max_length', truncation=True, max_length=32, return_tensors='pt') for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(tokenized_sentences, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "KEY = []\n",
    "DICT = {}\n",
    "\n",
    "def abn_loss(loss : list, k):\n",
    "    scores = torch.topk(torch.Tensor(loss), k)\n",
    "    return sum(scores.values.tolist())\n",
    "\n",
    "def abn_prob(probs, k):\n",
    "    scores = torch.topk(torch.Tensor(probs), k, largest=False)\n",
    "    return sum(scores.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.436710834503174 0.4099971130490303\n",
      "11.005641460418701 0.31468571349978447\n",
      "17.00260639190674 0.3297392353415489\n",
      "18.038761615753174 0.21984421834349632\n",
      "19.622427463531494 0.29583748430013657\n",
      "25.685500144958496 0.39338643103837967\n",
      "8.24725866317749 0.38908626511693\n",
      "13.110359191894531 0.22921616584062576\n",
      "8.224310636520386 0.36272698640823364\n",
      "3.711495280265808 0.501497071236372\n",
      "50.586917877197266 0.5157705321907997\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "for n in np.arange(0, len(tokenized_sentences)):\n",
    "    data = tokenized_sentences[n]\n",
    "    phrase = data.input_ids\n",
    "    data['labels'] = phrase.clone()\n",
    "    if phrase not in DICT:\n",
    "        score_loss, score_prob = [], []\n",
    "        with torch.no_grad():        \n",
    "            for i in np.arange(1, len(phrase[0])):\n",
    "                temp = phrase[0][i].item()\n",
    "                phrase[0][i] = 103\n",
    "\n",
    "                outputs = model(**data)\n",
    "\n",
    "                outputs_prob = max(torch.nn.functional.softmax(outputs.logits[0][i], dim=0))\n",
    "\n",
    "                score_loss.append(outputs.loss)\n",
    "                score_prob.append(outputs_prob)\n",
    "\n",
    "                #print(outputs.loss, outputs_prob)\n",
    "\n",
    "                \n",
    "                phrase[0][i] = temp\n",
    "        \n",
    "        agg_loss = abn_loss(score_loss, 3)\n",
    "        agg_prob = abn_prob(score_prob, 3)\n",
    "        #agg_prob = abn_prob(outputs.loss, 5)\n",
    "        KEY.append(phrase)\n",
    "        DICT.update({phrase : (agg_loss, agg_prob)})\n",
    "\n",
    "        print(agg_loss, agg_prob)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print('From Database', DICT[phrase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = DICT.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 17.436710834503174 Prob of Anomaly 0.4099971130490303 Anomaly Score 42.52886247132665\n",
      "Loss 11.005641460418701 Prob of Anomaly 0.31468571349978447 Anomaly Score 34.973438539739234\n",
      "Loss 17.00260639190674 Prob of Anomaly 0.3297392353415489 Anomaly Score 51.56379517376869\n",
      "Loss 18.038761615753174 Prob of Anomaly 0.21984421834349632 Anomaly Score 82.05247220815447\n",
      "Loss 19.622427463531494 Prob of Anomaly 0.29583748430013657 Anomaly Score 66.32840158829879\n",
      "Loss 25.685500144958496 Prob of Anomaly 0.39338643103837967 Anomaly Score 65.29330479742083\n",
      "Loss 8.24725866317749 Prob of Anomaly 0.38908626511693 Anomaly Score 21.19647852565288\n",
      "Loss 13.110359191894531 Prob of Anomaly 0.22921616584062576 Anomaly Score 57.1964858753033\n",
      "Loss 8.224310636520386 Prob of Anomaly 0.36272698640823364 Anomaly Score 22.673555993058308\n",
      "Loss 3.711495280265808 Prob of Anomaly 0.501497071236372 Anomaly Score 7.4008314168527995\n",
      "Loss 50.586917877197266 Prob of Anomaly 0.5157705321907997 Anomaly Score 98.08027934888607\n"
     ]
    }
   ],
   "source": [
    "for i, score in enumerate(scores):\n",
    "    anomaly_score = score[0] * 1/score[1]\n",
    "    print('Loss', score[0], 'Prob of Anomaly', score[1], 'Anomaly Score', anomaly_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1996,  3712,  2357,  1037, 15443,  4589,  2004,  1996,  3103,\n",
       "           103,  2917,  1996,  9154,  1010,  9179,  2146,  6281,  2408,  1996,\n",
       "         25283, 26147, 13244,  1012,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[  101,  1996,  3712,  2357,  1037, 15443,  4589,  2004,  1996,  3103,\n",
       "         13537,  2917,  1996,  9154,  1010,  9179,  2146,  6281,  2408,  1996,\n",
       "         25283, 26147, 13244,  1012,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0]])}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tokenized_sentences[0]\n",
    "test['labels'] = test.input_ids.detach().clone()\n",
    "test.input_ids[0, 10] = 103\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6932, grad_fn=<UnbindBackward0>)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**test)\n",
    "#print(outputs.loss)\n",
    "#print(outputs.logits.shape)\n",
    "#print(outputs.logits[0][10])\n",
    "proba = torch.nn.functional.softmax(outputs.logits[0][10], dim=0)\n",
    "print(max(proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9999, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(torch.nn.functional.softmax(outputs.logits[0][10], dim = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
